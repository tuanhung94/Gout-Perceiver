{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "249d2d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0b2364e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037e55fb",
   "metadata": {},
   "source": [
    "## Attention Scoring (Scaled Dot Product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8fdc0477",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, queries, keys, values):\n",
    "        \n",
    "        weights = torch.bmm(queries, torch.transpose(keys, 1, 2)) / math.sqrt(queries.shape[-1])\n",
    "        \n",
    "        self.attn_weights = F.softmax(weights, dim = -1)\n",
    "        \n",
    "        scores = torch.bmm(self.dropout(self.attn_weights), values)\n",
    "        \n",
    "        return scores    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f251ad",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f9de414",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(nn.Module):\n",
    "    \n",
    "    def __init__(self, norm_shape):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm = nn.LayerNorm(norm_shape)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        return self.norm(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2bdd62",
   "metadata": {},
   "source": [
    "## Residual Connection + Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "231bc8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    \n",
    "    def __init__(self, norm_shape):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm = nn.LayerNorm(norm_shape)\n",
    "        \n",
    "    def forward(self, inputs, outputs):\n",
    "        \n",
    "        return self.norm(inputs + outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f821dd3",
   "metadata": {},
   "source": [
    "## Initialize Latent Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d25dc301",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentArray(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_size, latent_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent_size = latent_size\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        latent_array = nn.Parameter(torch.rand(self.latent_size, self.latent_dim))\n",
    "        \n",
    "        torch.nn.init.trunc_normal_(latent_array, mean = 0.0, std = 0.02, a = -2.0, b = 2.0)\n",
    "        \n",
    "        batch_latent_array = latent_array.repeat(inputs.shape[0], 1, 1)\n",
    "        \n",
    "        return batch_latent_array   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28faa4f4",
   "metadata": {},
   "source": [
    "## Dense Multi-layer Perceptron Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd2caabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, bias = False):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm = nn.LayerNorm(input_dim)\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim, bias = bias)\n",
    "        \n",
    "        self.layer2 = nn.Linear(hidden_dim, input_dim, bias = bias)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # Layer Normalize the inputs\n",
    "        \n",
    "        outputs = self.norm(inputs)\n",
    "        \n",
    "        # Pass through the first linear layer & activate with GELU\n",
    "        \n",
    "        first_outputs = F.gelu(self.layer1(outputs))\n",
    "        \n",
    "        # Pass through the final linear layer\n",
    "        \n",
    "        final_outputs = self.layer2(first_outputs)\n",
    "        \n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea48ecfa",
   "metadata": {},
   "source": [
    "## Fully Connected FeedForward Neural Network Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "647b2a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedded_dim, hidden_size, output_size):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(embedded_dim, hidden_size)\n",
    "        \n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        first_outputs = F.relu(self.layer1(inputs))\n",
    "        \n",
    "        final_outputs = self.layer2(first_outputs)\n",
    "        \n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f37b880",
   "metadata": {},
   "source": [
    "## Multi Heads Attention (for Self-Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e78ae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dim, hidden_dim, num_heads, dropout, bias = False):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.attention = ScaledDotProductAttention(dropout)\n",
    "        \n",
    "        self.norm = Normalize(latent_dim)\n",
    "        \n",
    "        self.addnorm = AddNorm(latent_dim)\n",
    "        \n",
    "        self.queries_weights = nn.Linear(latent_dim, latent_dim, bias = bias)\n",
    "        \n",
    "        self.keys_weights = nn.Linear(latent_dim, latent_dim, bias = bias)\n",
    "        \n",
    "        self.values_weights = nn.Linear(latent_dim, latent_dim, bias = bias)\n",
    "        \n",
    "        self.heads_weights = nn.Linear(latent_dim, latent_dim, bias = bias)\n",
    "        \n",
    "        self.dense = DenseBlock(latent_dim, hidden_dim)\n",
    "        \n",
    "    def reshape_multi_heads(self, inputs):\n",
    "        \n",
    "        # batch size, number of queries/keys/values, number of heads, dimensions / number of heads\n",
    "        \n",
    "        inputs = inputs.reshape(inputs.shape[0], inputs.shape[1], self.num_heads, -1)\n",
    "        \n",
    "        # batch size, number of heads, number of queries/keys/values, dimensions / number of heads\n",
    "        \n",
    "        inputs = inputs.permute(0, 2, 1, 3)\n",
    "        \n",
    "        # batch size x number of heads, number of queries/keys/values, dimensions / number of heads\n",
    "\n",
    "        inputs = inputs.reshape(-1, inputs.shape[2], inputs.shape[3])\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    def reshape_output(self, outputs):\n",
    "        \n",
    "        # batch size, number of heads, number of queries/keys/values, dimensions / number of heads\n",
    "        \n",
    "        outputs = outputs.reshape(-1, self.num_heads, outputs.shape[1], outputs.shape[2])\n",
    "        \n",
    "        # batch size, number of queries/keys/values, number of heads, dimensions / number of heads\n",
    "\n",
    "        outputs = outputs.permute(0, 2, 1, 3)\n",
    "        \n",
    "        # batch size, number of queries/keys/values, dimensions\n",
    "        \n",
    "        outputs = outputs.reshape(outputs.shape[0], outputs.shape[1], -1)\n",
    "        \n",
    "        return outputs\n",
    "        \n",
    "    def forward(self, queries, keys, values):\n",
    "        \n",
    "        # Layer Normalize inputs\n",
    "        \n",
    "        norm_queries = self.norm(queries)\n",
    "        \n",
    "        norm_keys = self.norm(keys)\n",
    "        \n",
    "        norm_values = self.norm(values)\n",
    "        \n",
    "        # Re-shape QKV into multiple heads and attach learnable parameters\n",
    "        \n",
    "        new_queries = self.reshape_multi_heads(self.queries_weights(norm_queries))\n",
    "        \n",
    "        new_keys = self.reshape_multi_heads(self.keys_weights(norm_keys))\n",
    "        \n",
    "        new_values = self.reshape_multi_heads(self.values_weights(norm_values))\n",
    "        \n",
    "        # Perform attention scoring method\n",
    "        \n",
    "        outputs = self.attention(new_queries, new_keys, new_values)\n",
    "        \n",
    "        # Re-shape the outputs into their original shape (same with keys & values)\n",
    "\n",
    "        outputs = self.reshape_output(outputs)\n",
    "        \n",
    "        # Attach learnable parameters to heads\n",
    "        \n",
    "        outputs = self.heads_weights(outputs)\n",
    "        \n",
    "        # Residual Connection & Normalization\n",
    "        \n",
    "        outputs = self.addnorm(queries, outputs)\n",
    "        \n",
    "        # Pass through Dense Block\n",
    "        \n",
    "        final_outputs = self.dense(outputs)\n",
    "        \n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b704c43",
   "metadata": {},
   "source": [
    "## Cross Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e005f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedded_dim, hidden_dim, latent_dim, dropout, bias = False):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention = ScaledDotProductAttention(dropout)\n",
    "        \n",
    "        self.latent_norm = Normalize(latent_dim)\n",
    "        \n",
    "        self.norm = Normalize(embedded_dim)\n",
    "        \n",
    "        self.addnorm = AddNorm(embedded_dim)\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.keys_weights = nn.Linear(embedded_dim, embedded_dim, bias = bias)\n",
    "        \n",
    "        self.values_weights = nn.Linear(embedded_dim, embedded_dim, bias = bias)\n",
    "        \n",
    "        self.in_latent_linear = nn.Linear(latent_dim, embedded_dim, bias = bias)\n",
    "        \n",
    "        self.out_latent_linear = nn.Linear(embedded_dim, latent_dim, bias = bias)\n",
    "        \n",
    "        self.dense = DenseBlock(latent_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self, latent_array, keys, values):\n",
    "        \n",
    "        # Layer Normalize inputs\n",
    "        \n",
    "        norm_latent_array = self.latent_norm(latent_array)\n",
    "        \n",
    "        # Pass latent array to linear layer so its dimension is the same with keys & values\n",
    "        \n",
    "        latent_array_1 = self.in_latent_linear(norm_latent_array)\n",
    "        \n",
    "        # Normalize and attach learnable parameters to keys\n",
    "        \n",
    "        keys = self.norm(keys)\n",
    "        \n",
    "        keys = self.keys_weights(keys)\n",
    "        \n",
    "        # Normalize and attach learnable parameters to values\n",
    "        \n",
    "        values = self.norm(values)\n",
    "        \n",
    "        values = self.values_weights(values)\n",
    "        \n",
    "        # Perform cross attention for latent array with keys & values\n",
    "        \n",
    "        cross_outputs = self.attention(latent_array_1, keys, values)\n",
    "        \n",
    "        # Pass the outputs to a linear layer\n",
    "        \n",
    "        outputs = self.out_latent_linear(cross_outputs)\n",
    "        \n",
    "        # Return Residual Connection\n",
    "        \n",
    "        res_outputs = outputs + latent_array\n",
    "        \n",
    "        # Pass through Dense Block\n",
    "        \n",
    "        final_outputs = self.dense(res_outputs)\n",
    "        \n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fb8f2c",
   "metadata": {},
   "source": [
    "## Perceiver Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a710df94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceiverBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_heads, embedded_dim, hidden_dim, latent_dim, \n",
    "                 self_attention_modules, cross_attention_modules, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # self.cross_attention = CrossAttention(embedded_dim, dropout)\n",
    "        \n",
    "        # self.self_attention = MultiHeadAttention(embedded_dim, num_heads, dropout)\n",
    "        \n",
    "        self.addnorm = AddNorm(embedded_dim)\n",
    "        \n",
    "        self.linear_layer = nn.Linear(embedded_dim, embedded_dim)\n",
    "        \n",
    "        self.latent_linear = nn.Linear(latent_dim, embedded_dim)\n",
    "        \n",
    "        self.ffn = FFNLayer(latent_dim, hidden_dim, latent_dim)\n",
    "        \n",
    "        self.cross_modules = nn.Sequential()\n",
    "        \n",
    "        self.self_modules = nn.Sequential()\n",
    "        \n",
    "        for i in range(cross_attention_modules):\n",
    "            self.cross_modules.add_module(\"cross attention\"+str(i), CrossAttention(embedded_dim, hidden_dim, \n",
    "                                                                                   latent_dim, dropout))\n",
    "            \n",
    "        for i in range(self_attention_modules):\n",
    "            self.self_modules.add_module(\"self attention\"+str(i), MultiHeadAttention(latent_dim, hidden_dim,\n",
    "                                                                                     num_heads, dropout))\n",
    "        \n",
    "    def forward(self, latent_inputs, inputs):\n",
    "        \n",
    "        for i, cross_attention in enumerate(self.cross_modules):\n",
    "            \n",
    "            latent_inputs = cross_attention(latent_inputs, inputs, inputs)\n",
    "            \n",
    "        for i, self_attention in enumerate(self.self_modules):\n",
    "            \n",
    "            latent_inputs = self_attention(latent_inputs, latent_inputs, latent_inputs)\n",
    "              \n",
    "        return latent_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f69dd0",
   "metadata": {},
   "source": [
    "## Perceiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e88a2867",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceiver(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedded_dim, hidden_dim, num_heads, num_blocks, latent_size, latent_dim, \n",
    "                 self_attention_modules, cross_attention_modules, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.blocks = nn.Sequential()\n",
    "        \n",
    "        self.ffn = FFNLayer(latent_dim, hidden_dim, 10)\n",
    "        \n",
    "        self.latent = LatentArray(latent_size, latent_dim)\n",
    "        \n",
    "        self.norm = nn.LayerNorm(latent_dim)\n",
    "        \n",
    "        self.linear = nn.Linear(latent_dim, latent_dim)\n",
    "        \n",
    "        for i in range(num_blocks):\n",
    "            \n",
    "            self.blocks.add_module(\"block\"+str(i), PerceiverBlock(num_heads, embedded_dim, hidden_dim, latent_dim,\n",
    "                                                                  self_attention_modules, cross_attention_modules, dropout))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # Initialize Latent Array\n",
    "        \n",
    "        latent_outputs = self.latent(inputs).to(\"cuda:0\")\n",
    "        \n",
    "        # Running the Perceiver Blocks sequentially\n",
    "        \n",
    "        for i, perceiver_block in enumerate(self.blocks):\n",
    "            \n",
    "            latent_outputs = perceiver_block(latent_outputs, inputs)\n",
    "        \n",
    "        outputs = latent_outputs.mean(dim = 1)\n",
    "        \n",
    "        outputs_ffn = self.ffn(outputs)\n",
    "        \n",
    "        outputs_final = self.softmax(outputs_ffn)\n",
    "              \n",
    "        return outputs_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99df8dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing images\n",
    "\n",
    "image_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                       transforms.Normalize((0.5), (0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04dee270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading training and testing datasets\n",
    "\n",
    "train_data = torchvision.datasets.MNIST(root = './mnist_data', train = True,\n",
    "                                       download = False, transform = image_transforms)\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(root = './mnist_data', train = False,\n",
    "                                       download = False, transform = image_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a51172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading training and testing datasets\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d929ebeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_iter = iter(train_dataloader)\n",
    "\n",
    "images, labels = next(image_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4af690d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 28, 28])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6684d05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9f0da695",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Perceiver(embedded_dim = 28, num_heads = 4, hidden_dim = 32, num_blocks = 6, latent_size = 7, latent_dim = 36,\n",
    "                  self_attention_modules = 1, cross_attention_modules = 2, dropout = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fe2d447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001, amsgrad = True)\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b7104500",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a9c3d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total_steps = len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3e539f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "910e19fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [200/1875], Loss: 2.283\n",
      "Epoch [1/2], Step [400/1875], Loss: 2.130\n",
      "Epoch [1/2], Step [600/1875], Loss: 2.020\n",
      "Epoch [1/2], Step [800/1875], Loss: 2.035\n",
      "Epoch [1/2], Step [1000/1875], Loss: 1.921\n",
      "Epoch [1/2], Step [1200/1875], Loss: 1.999\n",
      "Epoch [1/2], Step [1400/1875], Loss: 1.956\n",
      "Epoch [1/2], Step [1600/1875], Loss: 1.974\n",
      "Epoch [1/2], Step [1800/1875], Loss: 1.808\n",
      "Epoch [2/2], Step [200/1875], Loss: 1.889\n",
      "Epoch [2/2], Step [400/1875], Loss: 1.836\n",
      "Epoch [2/2], Step [600/1875], Loss: 1.781\n",
      "Epoch [2/2], Step [800/1875], Loss: 1.790\n",
      "Epoch [2/2], Step [1000/1875], Loss: 1.809\n",
      "Epoch [2/2], Step [1200/1875], Loss: 1.832\n",
      "Epoch [2/2], Step [1400/1875], Loss: 1.945\n",
      "Epoch [2/2], Step [1600/1875], Loss: 1.764\n",
      "Epoch [2/2], Step [1800/1875], Loss: 1.591\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_dataloader):\n",
    "        \n",
    "        images = torch.squeeze(images, 1)\n",
    "        \n",
    "        # labels = torch.unsqueeze(labels, 1)\n",
    "        \n",
    "        images = images.to(device)\n",
    "        \n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 200 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e6eb2b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 317.4416432380676 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "20c4d443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of labels\n",
    "\n",
    "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0877d524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy of the model: 68.02 %\n",
      "Accuracy of Label 0 : 90.91836734693878 %\n",
      "Accuracy of Label 1 : 94.8898678414097 %\n",
      "Accuracy of Label 2 : 20.251937984496124 %\n",
      "Accuracy of Label 3 : 85.74257425742574 %\n",
      "Accuracy of Label 4 : 57.63747454175153 %\n",
      "Accuracy of Label 5 : 51.00896860986547 %\n",
      "Accuracy of Label 6 : 88.10020876826722 %\n",
      "Accuracy of Label 7 : 67.31517509727627 %\n",
      "Accuracy of Label 8 : 73.40862422997947 %\n",
      "Accuracy of Label 9 : 48.26560951437067 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained model performance\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    batch_size = 16\n",
    "    \n",
    "    num_correct_preds = 0\n",
    "    \n",
    "    num_total = len(test_data)\n",
    "    \n",
    "    num_correct_per_label = [0] * len(classes)\n",
    "    \n",
    "    num_total_per_label = [0] * len(classes)\n",
    "    \n",
    "    for images, labels in test_dataloader:\n",
    "        \n",
    "        images = torch.squeeze(images, 1)\n",
    "        \n",
    "        images = images.to(device)\n",
    "        \n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Return the value with the highest probability score\n",
    "        \n",
    "        _, pred_values = torch.max(outputs, 1)\n",
    "        \n",
    "        num_correct_preds += (pred_values == labels).sum().item()        \n",
    "    \n",
    "        for i in range(batch_size):\n",
    "            \n",
    "            label = labels[i]\n",
    "            \n",
    "            pred_val = pred_values[i]\n",
    "            \n",
    "            num_total_per_label[label] += 1 \n",
    "            \n",
    "            if label == pred_val:\n",
    "                \n",
    "                num_correct_per_label[label] += 1\n",
    "                \n",
    "    # Calculate Overall Accuracy\n",
    "    \n",
    "    overall_accuracy = 100.0 * num_correct_preds / num_total\n",
    "    \n",
    "    print(f'Overall accuracy of the model: {overall_accuracy} %')\n",
    "    \n",
    "    # Calculate Accuracy per Label\n",
    "    \n",
    "    for i in range(len(classes)):\n",
    "        \n",
    "        accuracy_per_label = 100.0 * num_correct_per_label[i] / num_total_per_label[i]\n",
    "        \n",
    "        print(f'Accuracy of Label {classes[i]} : {accuracy_per_label} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b19c9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reference\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    batch_size = 16\n",
    "    \n",
    "    num_correct_preds = 0\n",
    "    \n",
    "    num_total = batch_size * len(test_dataloader)\n",
    "    \n",
    "    num_class_correct = [0 for i in range(len(classes))]\n",
    "    \n",
    "    num_class_samples = [0 for i in range(len(classes))]\n",
    "    \n",
    "    for images, labels in test_dataloader:\n",
    "        \n",
    "        images = torch.squeeze(images, 1)\n",
    "        \n",
    "        images = images.to(device)\n",
    "        \n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Return the value with the highest probability score\n",
    "        \n",
    "        _, pred_values = torch.max(outputs, 1)\n",
    "        \n",
    "        num_correct_preds += (pred_values == labels).sum().item()\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            \n",
    "            label = labels[i]\n",
    "            \n",
    "            pred_val = pred_values[i]\n",
    "            \n",
    "            if (label == pred_val):\n",
    "                \n",
    "                num_class_correct[label] += 1\n",
    "                \n",
    "            num_class_samples[label] += 1\n",
    "\n",
    "    accuracy = 100.0 * num_correct_preds / num_total\n",
    "    \n",
    "    print(f'Accuracy of the network: {accuracy} %')\n",
    "\n",
    "    for i in range(len(classes)):\n",
    "        \n",
    "        accuracy = 100.0 * num_class_correct[i] / num_class_samples[i]\n",
    "        \n",
    "        print(f'Accuracy of {classes[i]}: {accuracy} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64966195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e01ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0546fe1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03564a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae901889",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
