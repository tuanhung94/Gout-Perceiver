{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "249d2d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "b0b2364e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037e55fb",
   "metadata": {},
   "source": [
    "## Attention Scoring (Scaled Dot Product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "8fdc0477",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, queries, keys, values):\n",
    "        \n",
    "        weights = torch.bmm(queries, torch.transpose(keys, 1, 2)) / math.sqrt(queries.shape[-1])\n",
    "        \n",
    "        self.attn_weights = F.softmax(weights, dim = -1)\n",
    "        \n",
    "        scores = torch.bmm(self.attn_weights, values)\n",
    "        \n",
    "        return scores    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f37b880",
   "metadata": {},
   "source": [
    "## Multi Heads Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "5e78ae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, dimensions, num_heads, bias = False):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.attention = ScaledDotProductAttention()\n",
    "        \n",
    "        self.queries_weights = nn.LazyLinear(dimensions, bias = bias)\n",
    "        \n",
    "        self.keys_weights = nn.LazyLinear(dimensions, bias = bias)\n",
    "        \n",
    "        self.values_weights = nn.LazyLinear(dimensions, bias = bias)\n",
    "        \n",
    "        self.heads_weights = nn.LazyLinear(dimensions, bias = bias)\n",
    "        \n",
    "    def reshape_multi_heads(self, inputs):\n",
    "        \n",
    "        # batch size, number of queries/keys/values, number of heads, dimensions / number of heads\n",
    "        \n",
    "        # 64 x 28 x 4 x 7\n",
    "        \n",
    "        inputs = inputs.reshape(inputs.shape[0], inputs.shape[1], self.num_heads, -1)\n",
    "        \n",
    "        # batch size, number of heads, number of queries/keys/values, dimensions / number of heads\n",
    "        \n",
    "        # 64 x 4 x 28 x 7\n",
    "        \n",
    "        inputs = inputs.permute(0, 2, 1, 3)\n",
    "        \n",
    "        # batch size x number of heads, number of queries/keys/values, dimensions / number of heads\n",
    "        \n",
    "        # 256 x 28 x 7 \n",
    "        \n",
    "        inputs = inputs.reshape(-1, inputs.shape[2], inputs.shape[3])\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    def reshape_output(self, outputs):\n",
    "        \n",
    "        # batch size, number of heads, number of queries/keys/values, dimensions / number of heads\n",
    "        \n",
    "        # 64 x 4 x 28 x 7\n",
    "        \n",
    "        outputs = outputs.reshape(-1, self.num_heads, outputs.shape[1], outputs.shape[2])\n",
    "        \n",
    "        # batch size, number of queries/keys/values, number of heads, dimensions / number of heads\n",
    "        \n",
    "        # 64 x 28 x 4 x 7\n",
    "        \n",
    "        outputs = outputs.permute(0, 2, 1, 3)\n",
    "        \n",
    "        # batch size, number of queries/keys/values, dimensions\n",
    "        \n",
    "        # 64 x 28 x 28\n",
    "        \n",
    "        outputs = outputs.reshape(outputs.shape[0], outputs.shape[1], -1)\n",
    "        \n",
    "        return outputs\n",
    "        \n",
    "    def forward(self, queries, keys, values):\n",
    "        \n",
    "        queries = self.reshape_multi_heads(self.queries_weights(queries))\n",
    "        \n",
    "        keys = self.reshape_multi_heads(self.keys_weights(keys))\n",
    "        \n",
    "        values = self.reshape_multi_heads(self.values_weights(values))\n",
    "        \n",
    "        outputs = self.attention(queries, keys, values)\n",
    "\n",
    "        outputs = self.reshape_output(outputs)\n",
    "        \n",
    "        outputs = self.heads_weights(outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d6ac32",
   "metadata": {},
   "source": [
    "## Initialize Latent Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "7c72b474",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentArray(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_size, latent_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent_size = latent_size\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        latent_array = nn.Parameter(torch.rand(self.latent_size, self.latent_dim))\n",
    "        \n",
    "        torch.nn.init.trunc_normal_(latent_array, mean = 0.0, std = 0.02, a = -2.0, b = 2.0)\n",
    "        \n",
    "        batch_latent_array = latent_array.repeat(inputs.shape[0], 1, 1)\n",
    "        \n",
    "        return batch_latent_array   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a994e457",
   "metadata": {},
   "source": [
    "# Perceiver Block (1 layer of Encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2bdd62",
   "metadata": {},
   "source": [
    "## Residual Connection + Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "231bc8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    \n",
    "    def __init__(self, norm_shape):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm = nn.LayerNorm(norm_shape)\n",
    "        \n",
    "    def forward(self, inputs, outputs):\n",
    "        \n",
    "        return self.norm(outputs + inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea48ecfa",
   "metadata": {},
   "source": [
    "## Fully Connected FeedForward Neural Network Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "647b2a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer1 = nn.LazyLinear(hidden_size)\n",
    "        \n",
    "        self.layer2 = nn.LazyLinear(output_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        first_outputs = F.relu(self.layer1(inputs))\n",
    "        \n",
    "        final_outputs = self.layer2(first_outputs)\n",
    "        \n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "99df8dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping files: 100%|█████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 69.81it/s]\n",
      "Mapping files: 100%|█████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 58.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proportion of 0's in the genomic dataset is 33.34 %\n",
      "The proportion of 1's in the genomic dataset is 33.34 %\n",
      "The proportion of 2's in the genomic dataset is 33.32 %\n",
      "The proportion of NaN's in the genomic dataset is 0.00 %\n",
      "The proportion of having gout is 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping files: 100%|█████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 47.65it/s]\n",
      "Mapping files: 100%|█████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 46.90it/s]\n",
      "C:\\Users\\tuanh\\AppData\\Local\\Temp\\ipykernel_19964\\2141829978.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['encoded_sex'] = np.select([df['sex'] == \"Male\" , df['sex'] == \"Female\"], [1, 2])\n",
      "C:\\Users\\tuanh\\AppData\\Local\\Temp\\ipykernel_19964\\2141829978.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['encoded_sex'] = np.select([df['sex'] == \"Male\" , df['sex'] == \"Female\"], [1, 2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6565656565656566\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False     0.6874    0.7312    0.7087      1131\n",
      "        True     0.6088    0.5571    0.5818       849\n",
      "\n",
      "    accuracy                         0.6566      1980\n",
      "   macro avg     0.6481    0.6442    0.6452      1980\n",
      "weighted avg     0.6537    0.6566    0.6543      1980\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tuanh\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "%run Data_Processing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a24d69a",
   "metadata": {},
   "source": [
    "## One Hot Encoding of the tokenised SNV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "b3e86178",
   "metadata": {},
   "outputs": [],
   "source": [
    "class One_Hot_Encoding(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes: int):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    def forward(self, input_ts):\n",
    "        \n",
    "        return F.one_hot(input_ts.long(), num_classes = self.num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686965ba",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "1f3cec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Positional_Encoding(nn.Module):\n",
    "    \n",
    "    def __init__(self, pos_dim, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.pos_dim = pos_dim\n",
    "        \n",
    "    def forward(self, input_ts):\n",
    "        \n",
    "        num_len = input_ts.shape[1]\n",
    "        \n",
    "        #num_dim = input_ts.shape[2]\n",
    "        \n",
    "        P = torch.zeros(1, num_len, self.pos_dim)\n",
    "        \n",
    "        X = (torch.arange(num_len, dtype=torch.float32).reshape(-1, 1) / \n",
    "        torch.pow(10000, torch.arange(0, self.pos_dim, 2, dtype = torch.float32) / self.pos_dim))\n",
    "        \n",
    "        P[:, :, 0::2] = torch.sin(X)\n",
    "        \n",
    "        if (self.pos_dim % 2) == 1:\n",
    "            \n",
    "            P[:, :, 1::2] = torch.cos(X[:, :-1])\n",
    "            \n",
    "        else:    \n",
    "            \n",
    "            P[:, :, 1::2] = torch.cos(X)\n",
    "        \n",
    "        P = torch.repeat_interleave(P, repeats = input_ts.shape[0], dim = 0)\n",
    "        \n",
    "        return self.dropout(P[:, :input_ts.shape[1], :].to(input_ts.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed38bf6",
   "metadata": {},
   "source": [
    "## Processing the raw datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "a33ffd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class prepare_dataset(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes: int, phenos_or_not: bool, pos_dim: int, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.one_hot_encoding = One_Hot_Encoding(num_classes)\n",
    "        \n",
    "        self.positional_encoding = Positional_Encoding(pos_dim, dropout)\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.phenos_or_not = phenos_or_not\n",
    "        \n",
    "    def forward(self, snv_ts, phenos_ts):\n",
    "        \n",
    "        # Process tokenised SNV data\n",
    "        \n",
    "        encoded_snv_ts = self.one_hot_encoding(snv_ts)\n",
    "        \n",
    "        pos_encoded_snv_ts = self.positional_encoding(encoded_snv_ts)\n",
    "        \n",
    "        if self.phenos_or_not:\n",
    "            \n",
    "            # Process phenotypes data\n",
    "            \n",
    "            phenos_ts = phenos_ts.unsqueeze(0)\n",
    "            \n",
    "            phenos_ts = torch.repeat_interleave(phenos_ts, repeats = snv_ts.shape[1], dim = 1)\n",
    "            \n",
    "            phenos_ts = phenos_ts.reshape(-1, snv_ts.shape[1], phenos_ts.shape[2])\n",
    "            \n",
    "            # Join encoded SNV and phenotypes datasets\n",
    "            \n",
    "            complete_ts = torch.cat((encoded_snv_ts, pos_encoded_snv_ts, phenos_ts), dim = 2)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            complete_ts = torch.cat((encoded_snv_ts, pos_encoded_snv_ts), dim = 2)\n",
    "            \n",
    "            # complete_ts = snv_ts.to(torch.float32)\n",
    "            \n",
    "        return complete_ts\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fb8f2c",
   "metadata": {},
   "source": [
    "## Perceiver Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "a710df94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceiverLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, dimensions, num_heads, hidden_ffn_size, cross_attention_modules, self_attention_modules):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttention(dimensions, num_heads)\n",
    "        \n",
    "        self.addnorm = AddNorm(dimensions)\n",
    "        \n",
    "        self.ffn = FFNLayer(hidden_ffn_size, dimensions)\n",
    "        \n",
    "        #self.cross_modules = nn.Sequential()\n",
    "        \n",
    "        #self.self_modules = nn.Sequential()\n",
    "        \n",
    "        self.cross_attn_mods = []\n",
    "        \n",
    "        self.self_attn_mods = []\n",
    "        \n",
    "        for i in range(cross_attention_modules):\n",
    "            \n",
    "            cross_mod = MultiHeadAttention(dimensions, num_heads, bias = False)\n",
    "            \n",
    "            self.cross_attn_mods.append(cross_mod)\n",
    "            \n",
    "        self.cross_attn_mods = nn.ModuleList(self.cross_attn_mods)\n",
    "            \n",
    "        for i in range(self_attention_modules):\n",
    "            \n",
    "            self_mod = MultiHeadAttention(dimensions, num_heads, bias = False)\n",
    "            \n",
    "            self.self_attn_mods.append(self_mod)\n",
    "            \n",
    "        self.self_attn_mods = nn.ModuleList(self.self_attn_mods)\n",
    "        \n",
    "        #for i in range(cross_attention_modules):\n",
    "        #    self.cross_modules.add_module(\"cross attention\"+str(i), MultiHeadAttention(dimensions, num_heads, bias = False))\n",
    "            \n",
    "        #for i in range(self_attention_modules):\n",
    "        #   self.self_modules.add_module(\"self attention\"+str(i), MultiHeadAttention(dimensions, num_heads, bias = False))\n",
    "        \n",
    "    def forward(self, latent_inputs, inputs):\n",
    "        \n",
    "        dup_inputs = latent_inputs.detach().clone()\n",
    "        \n",
    "        for cross_attention in self.cross_attn_mods:\n",
    "            \n",
    "            latent_inputs = cross_attention(latent_inputs, inputs, inputs)\n",
    "            \n",
    "        for self_attention in self.self_attn_mods:\n",
    "            \n",
    "            latent_inputs = self_attention(latent_inputs, latent_inputs, latent_inputs)\n",
    "        \n",
    "        outputs_norm = self.addnorm(dup_inputs, latent_inputs)\n",
    "        \n",
    "        outputs_ffn = self.ffn(outputs_norm)\n",
    "        \n",
    "        outputs_final = self.addnorm(outputs_norm, outputs_ffn)\n",
    "        \n",
    "        return outputs_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b495f413",
   "metadata": {},
   "source": [
    "## Perceiver Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "e88a2867",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceiver(nn.Module):\n",
    "    \n",
    "    def __init__(self, dimensions, num_heads, hidden_ffn_size, num_blocks, latent_size, latent_dim,\n",
    "                cross_attention_modules, self_attention_modules):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent = LatentArray(latent_size, latent_dim)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        #self.blocks = nn.Sequential()\n",
    "        \n",
    "        self.ffn = FFNLayer(hidden_ffn_size, 2)\n",
    "        \n",
    "        self.blocks = []\n",
    "        \n",
    "        for i in range(num_blocks):\n",
    "            \n",
    "            perceiver_block = PerceiverLayer(dimensions, num_heads, hidden_ffn_size,\n",
    "                                             cross_attention_modules, self_attention_modules)\n",
    "            \n",
    "            self.blocks.append(perceiver_block)\n",
    "            \n",
    "        self.blocks = nn.ModuleList(self.blocks)\n",
    "        \n",
    "        #for i in range(num_blocks):\n",
    "        #    self.blocks.add_module(\"block\"+str(i), PerceiverLayer(dimensions, num_heads, hidden_ffn_size,\n",
    "        #                                                         cross_attention_modules, self_attention_modules))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # Initialize Latent Array\n",
    "        \n",
    "        latent_inputs = self.latent(inputs).to(\"cuda:0\")\n",
    "        \n",
    "        for perceiver_block in self.blocks:\n",
    "            \n",
    "            inputs = perceiver_block(latent_inputs, inputs)\n",
    "            \n",
    "        outputs = inputs.reshape(inputs.shape[0], -1)\n",
    "            \n",
    "        # outputs = F.relu(inputs)\n",
    "        \n",
    "        outputs_ffn = F.relu(self.ffn(outputs))\n",
    "        \n",
    "        outputs_final = self.softmax(outputs_ffn)\n",
    "              \n",
    "        return outputs_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "04dee270",
   "metadata": {},
   "outputs": [],
   "source": [
    "pros = prepare_dataset(num_classes = 3, phenos_or_not = False, pos_dim = 10, dropout = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "17a62f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pros(tr_snv, tr_phenos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "6206b08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = train_df.unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "1ea6a7f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9899, 2000, 13])"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "060bb0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pros(te_snv, te_phenos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "a237ca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df = test_df.unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "21e91734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([101, 2000, 13])"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "9d0809aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9899])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_gout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "c4909233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([101])"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te_gout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "5e6b01cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data and labels\n",
    "\n",
    "train_ts = TensorDataset(train_df, tr_gout)\n",
    "\n",
    "test_ts = TensorDataset(test_df, te_gout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b83b88",
   "metadata": {},
   "source": [
    "## Test Run the Perceiver Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "9a51172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading training and testing datasets\n",
    "\n",
    "train_dataloader = DataLoader(train_ts, batch_size = 10, shuffle = True)\n",
    "\n",
    "#label_tr_dataloader = torch.utils.data.DataLoader(tr_gout, batch_size = 10, shuffle = False)\n",
    "\n",
    "test_dataloader = DataLoader(test_ts, batch_size = 4, shuffle = False)\n",
    "\n",
    "#label_te_dataloader = torch.utils.data.DataLoader(te_gout, batch_size = 4, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "2be2afea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tuanh\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "model = Perceiver(dimensions = 6, num_heads = 2, hidden_ffn_size = 32, num_blocks = 2, latent_size = 5,\n",
    "                 latent_dim = 6, self_attention_modules = 1, cross_attention_modules = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "fe2d447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-5, amsgrad = True)\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "b7104500",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "6a9c3d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total_steps = len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "3e539f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910e19fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [200/990], Loss: 0.693\n",
      "Epoch [1/5], Step [400/990], Loss: 0.693\n",
      "Epoch [1/5], Step [600/990], Loss: 0.693\n",
      "Epoch [1/5], Step [800/990], Loss: 0.693\n",
      "Epoch [2/5], Step [200/990], Loss: 0.693\n",
      "Epoch [2/5], Step [400/990], Loss: 0.693\n",
      "Epoch [2/5], Step [600/990], Loss: 0.698\n",
      "Epoch [2/5], Step [800/990], Loss: 0.693\n",
      "Epoch [3/5], Step [200/990], Loss: 0.693\n",
      "Epoch [3/5], Step [400/990], Loss: 0.693\n",
      "Epoch [3/5], Step [600/990], Loss: 0.693\n",
      "Epoch [3/5], Step [800/990], Loss: 0.693\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for i, (seqs, labels) in enumerate(train_dataloader):\n",
    "        \n",
    "        # labels = torch.unsqueeze(labels, 1)\n",
    "        \n",
    "        #images = torch.unsqueeze(images, 1)\n",
    "        \n",
    "        seqs = seqs.to(device)\n",
    "        \n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(seqs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 200 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eb2b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c4d443",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('0', '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b19c9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Evaluate the trained model performance\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    batch_size = 1\n",
    "    \n",
    "    num_correct_preds = 0\n",
    "    \n",
    "    num_total = len(test_df)\n",
    "    \n",
    "    num_correct_per_label = [0] * len(classes)\n",
    "    \n",
    "    num_total_per_label = [0] * len(classes)\n",
    "    \n",
    "    for i, (seqs, labels) in enumerate(test_dataloader):\n",
    "        \n",
    "        #images = torch.unsqueeze(images, 1)\n",
    "        \n",
    "        seqs = seqs.to(device)\n",
    "        \n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(seqs)\n",
    "        \n",
    "        # Return the value with the highest probability score\n",
    "        \n",
    "        _, pred_values = torch.max(outputs, 1)\n",
    "        \n",
    "        num_correct_preds += (pred_values == labels).sum().item()        \n",
    "    \n",
    "        for i in range(batch_size):\n",
    "            \n",
    "            label = labels[i]\n",
    "            \n",
    "            pred_val = pred_values[i]\n",
    "            \n",
    "            num_total_per_label[label] += 1 \n",
    "            \n",
    "            if label == pred_val:\n",
    "                \n",
    "                num_correct_per_label[label] += 1\n",
    "                \n",
    "    # Calculate Overall Accuracy\n",
    "    \n",
    "    overall_accuracy = 100.0 * num_correct_preds / num_total\n",
    "    \n",
    "    print(f'Overall accuracy of the model: {overall_accuracy:.3f} %')\n",
    "    \n",
    "    # Calculate Accuracy per Label\n",
    "    \n",
    "    for i in range(len(classes)):\n",
    "        \n",
    "        accuracy_per_label = 100.0 * num_correct_per_label[i] / num_total_per_label[i]\n",
    "        \n",
    "        print(f'Accuracy of Label {classes[i]} : {accuracy_per_label:.3f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac3077a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b86674",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30593af0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ff4214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cac6a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5375fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739d0438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
