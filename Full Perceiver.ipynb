{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1352,
   "id": "249d2d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1353,
   "id": "b0b2364e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1354,
   "id": "aa1ec6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping files: 100%|█████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 37.95it/s]\n",
      "Mapping files: 100%|█████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 42.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proportion of 0's in the genomic dataset is 33.34 %\n",
      "The proportion of 1's in the genomic dataset is 33.34 %\n",
      "The proportion of 2's in the genomic dataset is 33.32 %\n",
      "The proportion of NaN's in the genomic dataset is 0.00 %\n",
      "The proportion of having gout is 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping files: 100%|█████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 53.58it/s]\n",
      "Mapping files: 100%|█████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 49.77it/s]\n",
      "C:\\Users\\tuanh\\AppData\\Local\\Temp\\ipykernel_7648\\2141829978.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['encoded_sex'] = np.select([df['sex'] == \"Male\" , df['sex'] == \"Female\"], [1, 2])\n",
      "C:\\Users\\tuanh\\AppData\\Local\\Temp\\ipykernel_7648\\2141829978.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['encoded_sex'] = np.select([df['sex'] == \"Male\" , df['sex'] == \"Female\"], [1, 2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6449494949494949\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False     0.6969    0.7035    0.7002      1167\n",
      "        True     0.5686    0.5609    0.5647       813\n",
      "\n",
      "    accuracy                         0.6449      1980\n",
      "   macro avg     0.6328    0.6322    0.6325      1980\n",
      "weighted avg     0.6442    0.6449    0.6446      1980\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tuanh\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "%run Data_Processing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1355,
   "id": "78d47952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9899, 2000])"
      ]
     },
     "execution_count": 1355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_snv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1356,
   "id": "0544d49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9899, 5])"
      ]
     },
     "execution_count": 1356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_phenos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d23b03",
   "metadata": {},
   "source": [
    "## Attention Scoring (Scaled Dot Product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1357,
   "id": "8fdc0477",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, queries, keys, values):\n",
    "        \n",
    "        weights = torch.bmm(queries, torch.transpose(keys, 1, 2)) / math.sqrt(queries.shape[-1])\n",
    "        \n",
    "        self.attn_weights = F.softmax(weights, dim = -1)\n",
    "        \n",
    "        scores = torch.bmm(self.dropout(self.attn_weights), values)\n",
    "        \n",
    "        return scores    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f251ad",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1358,
   "id": "4f9de414",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(nn.Module):\n",
    "    \n",
    "    def __init__(self, norm_shape):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm = nn.LayerNorm(norm_shape)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        return self.norm(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2bdd62",
   "metadata": {},
   "source": [
    "## Residual Connection + Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1359,
   "id": "231bc8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    \n",
    "    def __init__(self, norm_shape):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm = nn.LayerNorm(norm_shape)\n",
    "        \n",
    "    def forward(self, inputs, outputs):\n",
    "        \n",
    "        return self.norm(inputs + outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f821dd3",
   "metadata": {},
   "source": [
    "## Initialize Latent Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1360,
   "id": "d25dc301",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentArray(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_size, latent_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent_size = latent_size\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        latent_array = nn.Parameter(torch.rand(self.latent_size, self.latent_dim))\n",
    "        \n",
    "        torch.nn.init.trunc_normal_(latent_array, mean = 0.0, std = 0.02, a = -2.0, b = 2.0)\n",
    "        \n",
    "        batch_latent_array = latent_array.repeat(inputs.shape[0], 1, 1)\n",
    "        \n",
    "        return batch_latent_array   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28faa4f4",
   "metadata": {},
   "source": [
    "## Dense Multi-layer Perceptron Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1361,
   "id": "fd2caabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, bias = False):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm = nn.LayerNorm(input_dim)\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim, bias = bias)\n",
    "        \n",
    "        self.layer2 = nn.Linear(hidden_dim, input_dim, bias = bias)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # Layer Normalize the inputs\n",
    "        \n",
    "        outputs = self.norm(inputs)\n",
    "        \n",
    "        # Pass through the first linear layer & activate with GELU\n",
    "        \n",
    "        first_outputs = F.gelu(self.layer1(outputs))\n",
    "        \n",
    "        # Pass through the final linear layer\n",
    "        \n",
    "        final_outputs = self.layer2(first_outputs)\n",
    "        \n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea48ecfa",
   "metadata": {},
   "source": [
    "## Fully Connected FeedForward Neural Network Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1362,
   "id": "647b2a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedded_dim, hidden_size, output_size):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(embedded_dim, hidden_size)\n",
    "        \n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        first_outputs = F.relu(self.layer1(inputs))\n",
    "        \n",
    "        final_outputs = self.layer2(first_outputs)\n",
    "        \n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f37b880",
   "metadata": {},
   "source": [
    "## Multi Heads Attention (for Self-Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1363,
   "id": "5e78ae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dim, hidden_dim, num_heads, dropout, bias = False):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.attention = ScaledDotProductAttention(dropout)\n",
    "        \n",
    "        self.norm = Normalize(latent_dim)\n",
    "        \n",
    "        self.addnorm = AddNorm(latent_dim)\n",
    "        \n",
    "        self.queries_weights = nn.Linear(latent_dim, latent_dim, bias = bias)\n",
    "        \n",
    "        self.keys_weights = nn.Linear(latent_dim, latent_dim, bias = bias)\n",
    "        \n",
    "        self.values_weights = nn.Linear(latent_dim, latent_dim, bias = bias)\n",
    "        \n",
    "        # self.heads_weights = nn.Linear(latent_dim, latent_dim, bias = bias)\n",
    "        \n",
    "        self.dense = DenseBlock(latent_dim, hidden_dim)\n",
    "        \n",
    "    def reshape_multi_heads(self, inputs):\n",
    "        \n",
    "        # batch size, number of queries/keys/values, number of heads, dimensions / number of heads\n",
    "        \n",
    "        inputs = inputs.reshape(inputs.shape[0], inputs.shape[1], self.num_heads, -1)\n",
    "        \n",
    "        # batch size, number of heads, number of queries/keys/values, dimensions / number of heads\n",
    "        \n",
    "        inputs = inputs.permute(0, 2, 1, 3)\n",
    "        \n",
    "        # batch size x number of heads, number of queries/keys/values, dimensions / number of heads\n",
    "\n",
    "        inputs = inputs.reshape(-1, inputs.shape[2], inputs.shape[3])\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    def reshape_output(self, outputs):\n",
    "        \n",
    "        # batch size, number of heads, number of queries/keys/values, dimensions / number of heads\n",
    "        \n",
    "        outputs = outputs.reshape(-1, self.num_heads, outputs.shape[1], outputs.shape[2])\n",
    "        \n",
    "        # batch size, number of queries/keys/values, number of heads, dimensions / number of heads\n",
    "\n",
    "        outputs = outputs.permute(0, 2, 1, 3)\n",
    "        \n",
    "        # batch size, number of queries/keys/values, dimensions\n",
    "        \n",
    "        outputs = outputs.reshape(outputs.shape[0], outputs.shape[1], -1)\n",
    "        \n",
    "        return outputs\n",
    "        \n",
    "    def forward(self, queries, keys, values):\n",
    "        \n",
    "        # Layer Normalize inputs\n",
    "        \n",
    "        norm_queries = self.norm(queries)\n",
    "        \n",
    "        norm_keys = self.norm(keys)\n",
    "        \n",
    "        norm_values = self.norm(values)\n",
    "        \n",
    "        # Re-shape QKV into multiple heads and attach learnable parameters\n",
    "        \n",
    "        new_queries = self.reshape_multi_heads(self.queries_weights(norm_queries))\n",
    "        \n",
    "        new_keys = self.reshape_multi_heads(self.keys_weights(norm_keys))\n",
    "        \n",
    "        new_values = self.reshape_multi_heads(self.values_weights(norm_values))\n",
    "        \n",
    "        # Perform attention scoring method\n",
    "        \n",
    "        outputs = self.attention(new_queries, new_keys, new_values)\n",
    "        \n",
    "        # Re-shape the outputs into their original shape (same with keys & values)\n",
    "\n",
    "        outputs = self.reshape_output(outputs)\n",
    "        \n",
    "        # Attach learnable parameters to heads\n",
    "        \n",
    "        #outputs = self.heads_weights(outputs)\n",
    "        \n",
    "        # Residual Connection & Normalization\n",
    "        \n",
    "        outputs = self.addnorm(queries, outputs)\n",
    "        \n",
    "        # Pass through Dense Block\n",
    "        \n",
    "        #final_outputs = self.dense(outputs)\n",
    "        \n",
    "        final_outputs = outputs\n",
    "        \n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b704c43",
   "metadata": {},
   "source": [
    "## Cross Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1364,
   "id": "9e005f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedded_dim, hidden_dim, latent_dim, dropout, bias = False):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention = ScaledDotProductAttention(dropout)\n",
    "        \n",
    "        self.latent_norm = Normalize(latent_dim)\n",
    "        \n",
    "        self.norm = Normalize(embedded_dim)\n",
    "        \n",
    "        self.addnorm = AddNorm(embedded_dim)\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.keys_weights = nn.Linear(embedded_dim, embedded_dim, bias = bias)\n",
    "        \n",
    "        self.values_weights = nn.Linear(embedded_dim, embedded_dim, bias = bias)\n",
    "        \n",
    "        self.in_latent_linear = nn.Linear(latent_dim, embedded_dim, bias = bias)\n",
    "        \n",
    "        self.out_latent_linear = nn.Linear(embedded_dim, latent_dim, bias = bias)\n",
    "        \n",
    "        self.dense = DenseBlock(latent_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self, latent_array, keys, values):\n",
    "        \n",
    "        # Layer Normalize inputs\n",
    "        \n",
    "        norm_latent_array = self.latent_norm(latent_array)\n",
    "        \n",
    "        # Pass latent array to linear layer so its dimension is the same with keys & values\n",
    "        \n",
    "        latent_array_1 = self.in_latent_linear(norm_latent_array)\n",
    "        \n",
    "        # Normalize and attach learnable parameters to keys\n",
    "        \n",
    "        keys = self.norm(keys)\n",
    "        \n",
    "        keys = self.keys_weights(keys)\n",
    "        \n",
    "        # Normalize and attach learnable parameters to values\n",
    "        \n",
    "        values = self.norm(values)\n",
    "        \n",
    "        values = self.values_weights(values)\n",
    "        \n",
    "        # Perform cross attention for latent array with keys & values\n",
    "        \n",
    "        cross_outputs = self.attention(latent_array_1, keys, values)\n",
    "        \n",
    "        # Pass the outputs to a linear layer\n",
    "        \n",
    "        outputs = self.out_latent_linear(cross_outputs)\n",
    "        \n",
    "        # Return Residual Connection\n",
    "        \n",
    "        res_outputs = outputs + latent_array\n",
    "        \n",
    "        # Pass through Dense Block\n",
    "        \n",
    "        #final_outputs = self.dense(res_outputs)\n",
    "        \n",
    "        final_outputs = res_outputs\n",
    "        \n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fb8f2c",
   "metadata": {},
   "source": [
    "## Perceiver Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1365,
   "id": "a710df94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceiverBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_heads, embedded_dim, hidden_dim, latent_dim, \n",
    "                 self_attention_modules, cross_attention_modules, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # self.cross_attention = CrossAttention(embedded_dim, dropout)\n",
    "        \n",
    "        # self.self_attention = MultiHeadAttention(embedded_dim, num_heads, dropout)\n",
    "        \n",
    "        self.addnorm = AddNorm(embedded_dim)\n",
    "        \n",
    "        self.linear_layer = nn.Linear(embedded_dim, embedded_dim)\n",
    "        \n",
    "        self.latent_linear = nn.Linear(latent_dim, embedded_dim)\n",
    "        \n",
    "        self.ffn = FFNLayer(latent_dim, hidden_dim, latent_dim)\n",
    "        \n",
    "        self.cross_modules = nn.Sequential()\n",
    "        \n",
    "        self.self_modules = nn.Sequential()\n",
    "        \n",
    "        for i in range(cross_attention_modules):\n",
    "            self.cross_modules.add_module(\"cross attention\"+str(i), CrossAttention(embedded_dim, hidden_dim, \n",
    "                                                                                   latent_dim, dropout))\n",
    "            \n",
    "        for i in range(self_attention_modules):\n",
    "            self.self_modules.add_module(\"self attention\"+str(i), MultiHeadAttention(latent_dim, hidden_dim,\n",
    "                                                                                     num_heads, dropout))\n",
    "        \n",
    "    def forward(self, latent_inputs, inputs):\n",
    "        \n",
    "        for i, cross_attention in enumerate(self.cross_modules):\n",
    "            \n",
    "            latent_inputs = cross_attention(latent_inputs, inputs, inputs)\n",
    "            \n",
    "        for i, self_attention in enumerate(self.self_modules):\n",
    "            \n",
    "            latent_inputs = self_attention(latent_inputs, latent_inputs, latent_inputs)\n",
    "              \n",
    "        return latent_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f69dd0",
   "metadata": {},
   "source": [
    "## Perceiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1366,
   "id": "e88a2867",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceiver(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedded_dim, hidden_dim, num_heads, num_blocks, latent_size, latent_dim, \n",
    "                 self_attention_modules, cross_attention_modules, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.blocks = nn.Sequential()\n",
    "        \n",
    "        self.ffn = FFNLayer(latent_dim, hidden_dim, 2)\n",
    "        \n",
    "        self.latent = LatentArray(latent_size, latent_dim)\n",
    "        \n",
    "        self.norm = nn.LayerNorm(latent_dim)\n",
    "        \n",
    "        self.linear = nn.Linear(latent_dim, latent_dim)\n",
    "        \n",
    "        for i in range(num_blocks):\n",
    "            \n",
    "            self.blocks.add_module(\"block\"+str(i), PerceiverBlock(num_heads, embedded_dim, hidden_dim, latent_dim,\n",
    "                                                                  self_attention_modules, cross_attention_modules, dropout))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # Initialize Latent Array\n",
    "        \n",
    "        latent_outputs = self.latent(inputs).to(\"cuda:0\")\n",
    "        \n",
    "        # Running the Perceiver Blocks sequentially\n",
    "        \n",
    "        for i, perceiver_block in enumerate(self.blocks):\n",
    "            \n",
    "            latent_outputs = perceiver_block(latent_outputs, inputs)\n",
    "        \n",
    "        outputs = latent_outputs.mean(dim = 1)\n",
    "        \n",
    "        outputs_ffn = self.ffn(outputs)\n",
    "        \n",
    "        outputs_final = self.softmax(outputs_ffn)\n",
    "              \n",
    "        return outputs_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de0c3d3",
   "metadata": {},
   "source": [
    "## One Hot Encoding of the tokenised SNV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1367,
   "id": "88840421",
   "metadata": {},
   "outputs": [],
   "source": [
    "class One_Hot_Encoding(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes: int):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    def forward(self, input_ts):\n",
    "        \n",
    "        return F.one_hot(input_ts.long(), num_classes = self.num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16212264",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1368,
   "id": "69c0bf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Positional_Encoding(nn.Module):\n",
    "    \n",
    "    def __init__(self, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input_ts):\n",
    "        \n",
    "        num_len = input_ts.shape[1]\n",
    "        \n",
    "        num_dim = input_ts.shape[2]\n",
    "        \n",
    "        P = torch.zeros(1, num_len, num_dim)\n",
    "        \n",
    "        X = (torch.arange(num_len, dtype=torch.float32).reshape(-1, 1) / \n",
    "        torch.pow(10000, torch.arange(0, num_dim, 2, dtype = torch.float32) / num_dim))\n",
    "        \n",
    "        P[:, :, 0::2] = torch.sin(X)\n",
    "        \n",
    "        if (num_dim % 2) == 1:\n",
    "            \n",
    "            P[:, :, 1::2] = torch.cos(X[:, :-1])\n",
    "            \n",
    "        else:    \n",
    "            \n",
    "            P[:, :, 1::2] = torch.cos(X)\n",
    "        \n",
    "        P = torch.repeat_interleave(P, repeats = input_ts.shape[0], dim = 0)\n",
    "        \n",
    "        return self.dropout(P[:, :input_ts.shape[1], :].to(input_ts.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe88e26",
   "metadata": {},
   "source": [
    "## Processing the dataset before feeding into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1369,
   "id": "03d3e23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class prepare_dataset(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes: int, phenos_or_not: bool, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.one_hot_encoding = One_Hot_Encoding(num_classes)\n",
    "        \n",
    "        self.positional_encoding = Positional_Encoding(dropout)\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.phenos_or_not = phenos_or_not\n",
    "        \n",
    "    def forward(self, snv_ts, phenos_ts):\n",
    "        \n",
    "        # Process tokenised SNV data\n",
    "        \n",
    "        encoded_snv_ts = self.one_hot_encoding(snv_ts)\n",
    "        \n",
    "        pos_encoded_snv_ts = self.positional_encoding(encoded_snv_ts)\n",
    "        \n",
    "        if self.phenos_or_not:\n",
    "            \n",
    "            # Process phenotypes data\n",
    "            \n",
    "            phenos_ts = phenos_ts.unsqueeze(0)\n",
    "            \n",
    "            phenos_ts = torch.repeat_interleave(phenos_ts, repeats = snv_ts.shape[1], dim = 1)\n",
    "            \n",
    "            phenos_ts = phenos_ts.reshape(-1, snv_ts.shape[1], phenos_ts.shape[2])\n",
    "            \n",
    "            # Join encoded SNV and phenotypes datasets\n",
    "            \n",
    "            complete_ts = torch.cat((encoded_snv_ts, pos_encoded_snv_ts, phenos_ts), dim = 2)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            complete_ts = torch.cat((encoded_snv_ts, pos_encoded_snv_ts), dim = 2)\n",
    "            \n",
    "            # complete_ts = snv_ts.to(torch.float32)\n",
    "            \n",
    "        return complete_ts\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1370,
   "id": "6504a964",
   "metadata": {},
   "outputs": [],
   "source": [
    "pros = prepare_dataset(num_classes = 3, phenos_or_not = False, dropout = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1371,
   "id": "c53bc1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pros(tr_snv, tr_phenos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1372,
   "id": "3f456177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 2,  ..., 0, 2, 2],\n",
       "        [2, 2, 0,  ..., 1, 1, 0],\n",
       "        [1, 1, 1,  ..., 2, 1, 0],\n",
       "        ...,\n",
       "        [0, 2, 2,  ..., 1, 1, 2],\n",
       "        [2, 2, 0,  ..., 2, 2, 1],\n",
       "        [1, 0, 1,  ..., 0, 2, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 1372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_snv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1373,
   "id": "842a6b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9899, 2000, 6])"
      ]
     },
     "execution_count": 1373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1374,
   "id": "7672bba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pros(te_snv, te_phenos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1375,
   "id": "94fa44cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([101, 2000, 6])"
      ]
     },
     "execution_count": 1375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1376,
   "id": "6afb94a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9899])"
      ]
     },
     "execution_count": 1376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_gout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1377,
   "id": "99df8dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing images\n",
    "\n",
    "#image_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "#                                       transforms.Normalize((0.5), (0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1378,
   "id": "04dee270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading training and testing datasets\n",
    "\n",
    "#train_data = torchvision.datasets.MNIST(root = './mnist_data', train = True,\n",
    "#                                       download = False, transform = image_transforms)\n",
    "\n",
    "#test_data = torchvision.datasets.MNIST(root = './mnist_data', train = False,\n",
    "#                                       download = False, transform = image_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1379,
   "id": "29d93ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading training and testing datasets\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_df, batch_size = 10, shuffle = False)\n",
    "\n",
    "label_tr_dataloader = torch.utils.data.DataLoader(tr_gout, batch_size = 10, shuffle = False)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_df, batch_size = 4, shuffle = False)\n",
    "\n",
    "label_te_dataloader = torch.utils.data.DataLoader(te_gout, batch_size = 4, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1392,
   "id": "9a51172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading training and testing datasets\n",
    "\n",
    "# train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "# test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1481,
   "id": "9f0da695",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Perceiver(embedded_dim = 2000, num_heads = 1, hidden_dim = 32, num_blocks = 1, latent_size = 4, latent_dim = 6,\n",
    "#                  self_attention_modules = 1, cross_attention_modules = 1, dropout = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1482,
   "id": "998bd422",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Perceiver(embedded_dim = 6, num_heads = 1, hidden_dim = 1, num_blocks = 1, latent_size = 6, latent_dim = 4,\n",
    "                  self_attention_modules = 1, cross_attention_modules = 1, dropout = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1483,
   "id": "fe2d447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 5e-7, amsgrad = True)\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1484,
   "id": "b7104500",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1485,
   "id": "6a9c3d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total_steps = len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1486,
   "id": "3e539f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1487,
   "id": "910e19fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [200/990], Loss: 0.912\n",
      "Epoch [1/2], Step [400/990], Loss: 0.769\n",
      "Epoch [1/2], Step [600/990], Loss: 0.721\n",
      "Epoch [1/2], Step [800/990], Loss: 0.626\n",
      "Epoch [2/2], Step [200/990], Loss: 0.912\n",
      "Epoch [2/2], Step [400/990], Loss: 0.769\n",
      "Epoch [2/2], Step [600/990], Loss: 0.721\n",
      "Epoch [2/2], Step [800/990], Loss: 0.626\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for i, (images, labels) in enumerate(zip(train_dataloader, label_tr_dataloader)):\n",
    "        \n",
    "        # labels = torch.unsqueeze(labels, 1)\n",
    "        \n",
    "        #images = torch.unsqueeze(images, 1)\n",
    "        \n",
    "        images = images.to(device)\n",
    "        \n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 200 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1488,
   "id": "e6eb2b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 36.452054262161255 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1489,
   "id": "20c4d443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of labels\n",
    "\n",
    "classes = ('0', '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1490,
   "id": "0877d524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy of the model: 58.416 %\n",
      "Accuracy of Label 0 : 100.000 %\n",
      "Accuracy of Label 1 : 0.000 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# Evaluate the trained model performance\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    batch_size = 1\n",
    "    \n",
    "    num_correct_preds = 0\n",
    "    \n",
    "    num_total = len(test_df)\n",
    "    \n",
    "    num_correct_per_label = [0] * len(classes)\n",
    "    \n",
    "    num_total_per_label = [0] * len(classes)\n",
    "    \n",
    "    for i, (images, labels) in enumerate(zip(test_dataloader, label_te_dataloader)):\n",
    "        \n",
    "        #images = torch.unsqueeze(images, 1)\n",
    "        \n",
    "        images = images.to(device)\n",
    "        \n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Return the value with the highest probability score\n",
    "        \n",
    "        _, pred_values = torch.max(outputs, 1)\n",
    "        \n",
    "        num_correct_preds += (pred_values == labels).sum().item()        \n",
    "    \n",
    "        for i in range(batch_size):\n",
    "            \n",
    "            label = labels[i]\n",
    "            \n",
    "            pred_val = pred_values[i]\n",
    "            \n",
    "            num_total_per_label[label] += 1 \n",
    "            \n",
    "            if label == pred_val:\n",
    "                \n",
    "                num_correct_per_label[label] += 1\n",
    "                \n",
    "    # Calculate Overall Accuracy\n",
    "    \n",
    "    overall_accuracy = 100.0 * num_correct_preds / num_total\n",
    "    \n",
    "    print(f'Overall accuracy of the model: {overall_accuracy:.3f} %')\n",
    "    \n",
    "    # Calculate Accuracy per Label\n",
    "    \n",
    "    for i in range(len(classes)):\n",
    "        \n",
    "        accuracy_per_label = 100.0 * num_correct_per_label[i] / num_total_per_label[i]\n",
    "        \n",
    "        print(f'Accuracy of Label {classes[i]} : {accuracy_per_label:.3f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2484f30e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
