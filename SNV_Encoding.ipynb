{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e649073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dbd7f7",
   "metadata": {},
   "source": [
    "# Encoding SNV data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e54c12",
   "metadata": {},
   "source": [
    "## - Encoding version 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c13b7eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_v6(string_to_tok, tok_to_string, a0, a1, pos: int, length: int):\n",
    "    \n",
    "    a0_toks = np.zeros(length, dtype = np.int32)\n",
    "    \n",
    "    a1_toks = np.zeros(length, dtype = np.int32)\n",
    "    \n",
    "    a01_toks = np.zeros(length, dtype = np.int32)\n",
    "    \n",
    "    for i, (a,b) in enumerate(zip(a0, a1)):\n",
    "        \n",
    "        if len(a) == 1 & len(b) == 1:\n",
    "            \n",
    "            a_string = str(a)\n",
    "            \n",
    "            b_string = str(b)\n",
    "            \n",
    "            ab_string = a + \",\" + b\n",
    "            \n",
    "        elif len(a) > len(b):\n",
    "            \n",
    "            a_string = \"longer\"\n",
    "            \n",
    "            b_string = \"shorter\"\n",
    "            \n",
    "            ab_string = \"mixed_indel\"\n",
    "            \n",
    "        elif len(a) < len(b):\n",
    "            \n",
    "            a_string = \"shorter\"\n",
    "            \n",
    "            b_string = \"longer\"\n",
    "            \n",
    "            ab_string = \"mixed_indel\"\n",
    "            \n",
    "        elif (len(a) == len(b)) and len(a) > 1:\n",
    "            \n",
    "            a_string = \"long_sub\"\n",
    "            \n",
    "            b_string = \"long_sub\"\n",
    "            \n",
    "            ab_string = \"mixed_long_sub\"\n",
    "            \n",
    "        # Obtain token for 2 major alleles\n",
    "            \n",
    "        if a_string not in string_to_tok:\n",
    "            \n",
    "            string_to_tok[a_string] = pos\n",
    "            \n",
    "            tok_to_string[pos] = a_string\n",
    "            \n",
    "            a_tok = pos\n",
    "            \n",
    "            pos += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            a_tok = string_to_tok[a_string]\n",
    "            \n",
    "        # Obtain token for 2 minor alleles\n",
    "            \n",
    "        if b_string not in string_to_tok:\n",
    "            \n",
    "            string_to_tok[b_string] = pos\n",
    "            \n",
    "            tok_to_string[pos] = b_string\n",
    "            \n",
    "            b_tok = pos\n",
    "            \n",
    "            pos += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            b_tok = string_to_tok[b_string]\n",
    "            \n",
    "        # Obtain token for 1 major allele & 1 minor allele\n",
    "            \n",
    "        if ab_string not in string_to_tok:\n",
    "            \n",
    "            string_to_tok[ab_string] = pos\n",
    "            \n",
    "            tok_to_string[pos] = ab_string\n",
    "            \n",
    "            ab_tok = pos\n",
    "            \n",
    "            pos += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            ab_tok = string_to_tok[ab_string]\n",
    "            \n",
    "        a0_toks[i] = a_tok\n",
    "        \n",
    "        a1_toks[i] = b_tok\n",
    "        \n",
    "        a01_toks[i] = ab_tok\n",
    "        \n",
    "    return a0_toks, a1_toks, a01_toks, string_to_tok, tok_to_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8c205d",
   "metadata": {},
   "source": [
    "## - Encoding version 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d56f2848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_v5(string_to_tok, tok_to_string, a0, a1, pos: int, length: int):\n",
    "    \n",
    "    for i in [0, 1, 2]:\n",
    "        \n",
    "        string_to_tok[str(i)] = i\n",
    "        \n",
    "        tok_to_string[i] = str(i)\n",
    "        \n",
    "    a0_toks = np.array(np.repeat(0, len(a0)), dtype = np.int32)\n",
    "    \n",
    "    a1_toks = np.array(np.repeat(2, len(a1)), dtype = np.int32)\n",
    "    \n",
    "    a01_toks = np.array(np.repeat(1, len(a0)), dtype = np.int32)\n",
    "    \n",
    "    return a0_toks, a1_toks, a01_toks, string_to_tok, tok_to_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156349ce",
   "metadata": {},
   "source": [
    "## - Encoding version 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8f00f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_v4(string_to_tok, tok_to_string, a0, a1, pos: int, length: int):\n",
    "    \n",
    "    a0_toks = np.zeros(length, dtype = np.int32)\n",
    "    \n",
    "    a1_toks = np.zeros(length, dtype = np.int32)\n",
    "    \n",
    "    a01_toks = np.zeros(length, dtype = np.int32)\n",
    "    \n",
    "    diff_lens = np.zeros(length, dtype = np.int32)\n",
    "    \n",
    "    for i, (a,b) in enumerate(zip(a0, a1)):\n",
    "            \n",
    "        a_string = str(a)\n",
    "            \n",
    "        b_string = str(b)\n",
    "            \n",
    "        if len(a) > 1:\n",
    "            \n",
    "            a_string = \"seq\"\n",
    "            \n",
    "        if len(b) > 1:\n",
    "            \n",
    "            b_string = \"seq\"\n",
    "            \n",
    "        ab_string = a_string + \",\" + b_string\n",
    "        \n",
    "        # Major allele a0\n",
    "        \n",
    "        if a_string not in string_to_tok:\n",
    "            \n",
    "            a_tok = pos\n",
    "            \n",
    "            string_to_tok[a_string] = pos\n",
    "            \n",
    "            tok_to_string[pos] = a_string\n",
    "            \n",
    "            pos += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            a_tok = string_to_tok[a_string]\n",
    "            \n",
    "        # Minor allele a1\n",
    "            \n",
    "        if b_string not in string_to_tok:\n",
    "            \n",
    "            b_tok = pos\n",
    "            \n",
    "            string_to_tok[b_string] = pos\n",
    "            \n",
    "            tok_to_string[pos] = b_string\n",
    "            \n",
    "            pos += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            b_tok = string_to_tok[b_string]\n",
    "        \n",
    "        # Major allele a0 & Minor Allele a1\n",
    "        \n",
    "        if ab_string not in string_to_tok:\n",
    "            \n",
    "            ab_tok = pos\n",
    "            \n",
    "            string_to_tok[ab_string] = pos\n",
    "            \n",
    "            tok_to_string[pos] = ab_string\n",
    "            \n",
    "            pos += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            ab_tok = string_to_tok[ab_string]\n",
    "            \n",
    "        # Add token\n",
    "            \n",
    "        a0_toks[i] = a_tok\n",
    "        \n",
    "        a1_toks[i] = b_tok\n",
    "        \n",
    "        a01_toks[i] = ab_tok\n",
    "        \n",
    "        diff_lens[i] = len(b) - len(a)\n",
    "        \n",
    "    return a0_toks, a1_toks, a01_toks, string_to_tok, tok_to_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6701576",
   "metadata": {},
   "source": [
    "## - Encoding version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0597f795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_v3(string_to_tok, tok_to_string, a0, a1, pos: int, length: int):\n",
    "    \n",
    "    a0_toks = np.zeros(length, dtype = np.int32)\n",
    "    \n",
    "    a1_toks = np.zeros(length, dtype = np.int32)\n",
    "    \n",
    "    a01_toks = np.zeros(length, dtype = np.int32)\n",
    "    \n",
    "    for i, (a,b) in enumerate(zip(a0, a1)):\n",
    "        \n",
    "        a_string = str(a)\n",
    "        \n",
    "        b_string = str(b)\n",
    "        \n",
    "        if len(a) > 1:\n",
    "            \n",
    "            a_string = a_string[0] + \"I\"\n",
    "            \n",
    "        if len(b) > 1:\n",
    "            \n",
    "            b_string = b_string[0] + \"I\"\n",
    "            \n",
    "        if len(str(a)) <= len(str(b)):\n",
    "            \n",
    "            ab_string = a_string + \",\" + b_string\n",
    "            \n",
    "        elif len(str(a)) > len(str(b)):\n",
    "            \n",
    "            ab_string = a_string + \",\" + \"del\"\n",
    "            \n",
    "        # Major allele a0\n",
    "        \n",
    "        if a_string not in string_to_tok:\n",
    "            \n",
    "            a_tok = pos\n",
    "            \n",
    "            string_to_tok[a_string] = pos\n",
    "            \n",
    "            tok_to_string[pos] = a_string\n",
    "            \n",
    "            pos += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            a_tok = string_to_tok[a_string]\n",
    "            \n",
    "        # Minor allele a1\n",
    "        \n",
    "        if b_string not in string_to_tok:\n",
    "            \n",
    "            b_tok = pos\n",
    "            \n",
    "            string_to_tok[b_string] = pos\n",
    "            \n",
    "            tok_to_string[pos] = b_string\n",
    "            \n",
    "            pos += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            b_tok = string_to_tok[b_string]\n",
    "            \n",
    "        # Major allele a0 & Minor allele a1\n",
    "        \n",
    "        if ab_string not in string_to_tok:\n",
    "            \n",
    "            ab_tok = pos\n",
    "            \n",
    "            string_to_tok[ab_string] = pos\n",
    "            \n",
    "            tok_to_string[pos] = ab_string\n",
    "            \n",
    "            pos += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            ab_tok = string_to_tok[ab_string]\n",
    "            \n",
    "        a0_toks[i] = a_tok\n",
    "        \n",
    "        a1_toks[i] = b_tok\n",
    "        \n",
    "        a01_toks[i] = ab_tok\n",
    "        \n",
    "    return a0_toks, a1_toks, a01_toks, string_to_tok, tok_to_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5411c9",
   "metadata": {},
   "source": [
    "## - Encoding version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e790c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_v2(string_to_tok, tok_to_string, a0, a1, pos: int, length: int):\n",
    "    \n",
    "    a0_toks = np.zeros(length, dtype = np.int32)\n",
    "    \n",
    "    a1_toks = np.zeros(length, dtype = np.int32)\n",
    "    \n",
    "    a01_toks = np.zeros(length, dtype = np.int32)\n",
    "    \n",
    "    # Encoding a0\n",
    "    \n",
    "    for i, string in enumerate(a0):\n",
    "        \n",
    "        if len(string) > 1:\n",
    "            \n",
    "            string = string[0] + \"I\"\n",
    "            \n",
    "        if string not in string_to_tok:\n",
    "            \n",
    "            string_to_tok[string] = pos\n",
    "            \n",
    "            tok_to_string[pos] = string\n",
    "            \n",
    "            a_tok = pos\n",
    "            \n",
    "            pos += 1\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            a_tok = string_to_tok[string]\n",
    "            \n",
    "        a0_toks[i] = a_tok\n",
    "        \n",
    "    # Encoding a1\n",
    "    \n",
    "    for i in range(len(a1)):\n",
    "        \n",
    "        if len(a1[i]) == len(a0[i]):\n",
    "            \n",
    "            if len(a1[i]) > 1:\n",
    "                \n",
    "                string = a1[i] + \"I\"\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                string = a1[i]\n",
    "                \n",
    "        elif len(a1[i]) > len(a0[i]):\n",
    "            \n",
    "            string = \"ins\"\n",
    "            \n",
    "        elif len(a1[i]) < len(a0[i]):\n",
    "            \n",
    "            string = \"del\"\n",
    "            \n",
    "        if string not in string_to_tok:\n",
    "            \n",
    "            string_to_tok[string] = pos\n",
    "            \n",
    "            tok_to_string[pos] = string\n",
    "            \n",
    "            b_tok = pos\n",
    "            \n",
    "            pos += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            b_tok = string_to_tok[string]\n",
    "            \n",
    "        a1_toks[i] = b_tok\n",
    "        \n",
    "    # Encoding a01\n",
    "        \n",
    "    for i, (a,b) in enumerate(zip(a0, a1)):\n",
    "        \n",
    "        a = str(a)\n",
    "        \n",
    "        b = str(b)\n",
    "        \n",
    "        if len(a) > 1:\n",
    "            \n",
    "            a = a + \"I\"\n",
    "            \n",
    "        if len(b) > 1:\n",
    "            \n",
    "            b = b + \"I\"\n",
    "        \n",
    "        if len(a) == len(b):\n",
    "            \n",
    "            string = a + b\n",
    "            \n",
    "        elif len(a) > len(b):\n",
    "            \n",
    "            string = a + \",\" + \"del\"\n",
    "        \n",
    "        elif len(a) < len(b):\n",
    "            \n",
    "            string = a + \",\" + \"ins\"\n",
    "            \n",
    "        if string not in string_to_tok:\n",
    "            \n",
    "            string_to_tok[string] = pos\n",
    "            \n",
    "            tok_to_string[pos] = string\n",
    "            \n",
    "            ab_tok = pos\n",
    "            \n",
    "            pos += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            ab_tok = string_to_tok[string]\n",
    "            \n",
    "        a01_toks[i] = ab_tok\n",
    "        \n",
    "    return a0_toks, a1_toks, a01_toks, string_to_tok, tok_to_string\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2663a370",
   "metadata": {},
   "source": [
    "## - Encoding version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e2d99e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_v2(string_to_tok, tok_to_string, a0, a1, pos: int, length: int):\n",
    "    \n",
    "    a0_toks = np.zeros(length, dtype = np.int32)\n",
    "    \n",
    "    a1_toks = np.zeros(length, dtype = np.int32)\n",
    "    \n",
    "    a01_toks = np.zeros(length, dtype = np.int32)\n",
    "    \n",
    "    # Encoding a0\n",
    "    \n",
    "    for i, string in enumerate(a0):\n",
    "        \n",
    "        if string not in string_to_tok:\n",
    "            \n",
    "            string_to_tok[string] = pos\n",
    "            \n",
    "            tok_to_string[pos] = string\n",
    "            \n",
    "            tok = pos\n",
    "            \n",
    "            pos += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            tok = string_to_tok[string]\n",
    "            \n",
    "        a0_toks[i] = tok\n",
    "        \n",
    "    # Encoding a1\n",
    "        \n",
    "    for i, string in enumerate(a1):\n",
    "        \n",
    "        if string not in string_to_tok:\n",
    "            \n",
    "            string_to_tok[string] = pos\n",
    "            \n",
    "            tok_to_string[pos] = string\n",
    "            \n",
    "            tok = pos\n",
    "            \n",
    "            pos += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            tok = string_to_tok[string]\n",
    "            \n",
    "        a1_toks[i] = tok\n",
    "        \n",
    "    # Encoding a01\n",
    "    \n",
    "    for i, (a,b) in enumerate(zip(a0, a1)):\n",
    "        \n",
    "        string = str(a) + ',' + str(b)\n",
    "        \n",
    "        if string not in string_to_tok:\n",
    "            \n",
    "            string_to_tok[string] = pos\n",
    "            \n",
    "            tok_to_string[pos] = string\n",
    "            \n",
    "            tok = pos\n",
    "            \n",
    "            pos += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            tok = string_to_tok[string]\n",
    "            \n",
    "        a01_toks[i] = tok\n",
    "        \n",
    "    return a0_toks, a1_toks, a01_toks, string_to_tok, tok_to_string\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721bb906",
   "metadata": {},
   "source": [
    "## Obtain the token matrix for SNV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f10b393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_matrix(geno, encoding : int = 2):\n",
    "    \n",
    "    a0 = geno.a0.values\n",
    "    \n",
    "    a1 = geno.a1.values\n",
    "    \n",
    "    string_to_token = {}\n",
    "    \n",
    "    token_to_string = {}\n",
    "    \n",
    "    pos = 0\n",
    "    \n",
    "    for special_token in ['nan', 'del', 'ins']:\n",
    "        \n",
    "        string_to_token[special_token] = pos\n",
    "        \n",
    "        token_to_string[pos] = special_token\n",
    "        \n",
    "        pos += 1\n",
    "        \n",
    "        geno_length = geno.shape[0]\n",
    "        \n",
    "        geno_size = geno.shape[1]\n",
    "        \n",
    "    if encoding == 1:\n",
    "        \n",
    "        a0_toks, a1_toks, a01_toks, string_to_tok, tok_to_string = enc_v1(string_to_token, token_to_string,\n",
    "                                                                      a0, a1, pos, geno_size)\n",
    "    \n",
    "    elif encoding == 2:\n",
    "        \n",
    "        a0_toks, a1_toks, a01_toks, string_to_tok, tok_to_string = enc_v2(string_to_token, token_to_string,\n",
    "                                                                      a0, a1, pos, geno_size)\n",
    "    \n",
    "    elif encoding == 3:\n",
    "        \n",
    "        a0_toks, a1_toks, a01_toks, string_to_tok, tok_to_string = enc_v3(string_to_token, token_to_string,\n",
    "                                                                      a0, a1, pos, geno_size)\n",
    "    \n",
    "    elif encoding == 4:\n",
    "        \n",
    "        a0_toks, a1_toks, a01_toks, string_to_tok, tok_to_string = enc_v4(string_to_token, token_to_string,\n",
    "                                                                      a0, a1, pos, geno_size)\n",
    "    \n",
    "    elif encoding == 5:\n",
    "        \n",
    "        a0_toks, a1_toks, a01_toks, string_to_tok, tok_to_string = enc_v5(string_to_token, token_to_string,\n",
    "                                                                      a0, a1, pos, geno_size)\n",
    "    \n",
    "    elif encoding == 6:\n",
    "        \n",
    "        a0_toks, a1_toks, a01_toks, string_to_tok, tok_to_string = enc_v6(string_to_token, token_to_string,\n",
    "                                                                      a0, a1, pos, geno_size)\n",
    "    \n",
    "    # Initialize token matrix for further processing\n",
    "    \n",
    "    batch_size = 1024\n",
    "    \n",
    "    token_matrix = np.zeros(shape = (geno_length, geno_size), dtype = np.int32)\n",
    "    \n",
    "    diff_alleles_matrix = np.zeros(shape = (geno_length, geno_size), dtype = np.bool_)\n",
    "    \n",
    "    non_ref_alleles_matrix = np.zeros(shape = (geno_length, geno_size), dtype = np.bool_)\n",
    "    \n",
    "    num_batches = np.ceil(geno_length / batch_size)\n",
    "    \n",
    "    for num_batch in range(int(num_batches)):\n",
    "        \n",
    "        geno_mat = np.array(geno[num_batch * batch_size : (num_batch + 1) * batch_size].values, dtype = np.int32)\n",
    "        \n",
    "        for num_row in range(geno_mat.shape[0]):\n",
    "            \n",
    "            # Make sure row index does not exceed the last row\n",
    "            \n",
    "            actual_row_num = num_batch * batch_size + num_row\n",
    "            \n",
    "            if actual_row_num < geno_length:\n",
    "            \n",
    "                for num_col in range(geno_size):\n",
    "                \n",
    "                    alleles = geno_mat[num_row, num_col]\n",
    "\n",
    "                    if encoding != 4:\n",
    "                \n",
    "                        if alleles == 0:\n",
    "                    \n",
    "                            token = a0_toks[num_col]\n",
    "                        \n",
    "                        elif alleles == 1:\n",
    "                        \n",
    "                            token = a01_toks[num_col]\n",
    "                        \n",
    "                        elif alleles == 2:\n",
    "                        \n",
    "                            token = a1_toks[num_col]\n",
    "                        \n",
    "                        else:\n",
    "                        \n",
    "                            token = string_to_tok['nan']\n",
    "                        \n",
    "                    elif encoding == 4:\n",
    "                    \n",
    "                        if alleles == \"0\":\n",
    "                        \n",
    "                            token = a0_toks[num_col]\n",
    "                        \n",
    "                            diff_alleles_matrix[actual_row_num, num_col] = 0\n",
    "                        \n",
    "                            non_ref_alleles_matrix[actual_row_num, num_col] = 0\n",
    "                        \n",
    "                        elif alleles == \"1\":\n",
    "                        \n",
    "                            token = a01_toks[num_col]\n",
    "                        \n",
    "                            diff_alleles_matrix[actual_row_num, num_col] = 1\n",
    "                        \n",
    "                            non_ref_alleles_matrix[actual_row_num, num_col] = 1\n",
    "                        \n",
    "                        elif alleles == \"2\":\n",
    "                        \n",
    "                            token = a1_toks[num_col]\n",
    "                        \n",
    "                            diff_alleles_matrix[actual_row_num, num_col] = 0\n",
    "                        \n",
    "                            non_ref_alleles_matrix[actual_row_num, num_col] = 1\n",
    "                     \n",
    "                        else:\n",
    "                        \n",
    "                            token = string_to_tok['nan']\n",
    "                        \n",
    "                    token_matrix[actual_row_num, num_col] = token\n",
    "                \n",
    "    token_matrix = torch.from_numpy(token_matrix)\n",
    "    \n",
    "    if encoding == 4:\n",
    "        \n",
    "        return token_matrix, diff_alleles_matrix, non_ref_alleles_matrix, diff_lens, string_to_tok, tok_to_string, len(string_to_tok)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return token_matrix, string_to_tok, tok_to_string, len(string_to_tok)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f25ae14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_plink import read_plink1_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5aa7ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping files: 100%|█████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 66.71it/s]\n"
     ]
    }
   ],
   "source": [
    "bed_file = \"test_logical.bed\"\n",
    "    \n",
    "bim_file = \"test_logical.bim\"\n",
    "    \n",
    "fam_file = \"test_logical.fam\"\n",
    "    \n",
    "# Load in genomic files containing SNV data\n",
    "    \n",
    "test_geno = read_plink1_bin(bed_file, bim_file, fam_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d950a61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c, d = get_token_matrix(test_geno, encoding = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cda357a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 2,  ..., 0, 2, 2],\n",
       "        [2, 2, 0,  ..., 1, 1, 0],\n",
       "        [1, 1, 1,  ..., 2, 1, 0],\n",
       "        ...,\n",
       "        [0, 2, 2,  ..., 1, 1, 2],\n",
       "        [2, 2, 0,  ..., 2, 2, 1],\n",
       "        [1, 0, 1,  ..., 0, 2, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9501bef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nan': 0, 'del': 1, 'ins': 2, '0': 0, '1': 1, '2': 2}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ca633e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '0', 1: '1', 2: '2'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4905097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81b63c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6d2993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34071f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11755c66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
